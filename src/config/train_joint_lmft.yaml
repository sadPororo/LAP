# -----------------------------------------
# Common configurations
# -----------------------------------------
seed: 42
ncpu: 4
epochs: 3
keep_nbest: 3
train_max_duration: 6.
n_iter_per_epoch: 2000
batch_size: 128
grad_acc: 4
verbose: True

# -----------------------------------------
# Stochastic model average
# -----------------------------------------
apply_swa: False
swa_lr: 0.000005
swa_decay: 0.9
swa_anneal_epochs: 2
swa_anneal_strategy: 'cos'

# ------------------------------------------------
# Choose N-th best model from previous phase
# ------------------------------------------------
choose_topN: 1

# ------------------------------------------------
# AAM-softmax loss configurations (Large Margin Finetuning)
# ------------------------------------------------
margin: 0.5
topK: 0
margin_penalty: 0.0

# ------------------------------------------------
# Adam - optimizer configurations
# ------------------------------------------------
lr: 0.00005
weight_decay: 0.00001
max_norm: 9999. # gradient clipping

# ------------------------------------------------
# CosineAnnealingWarmupRestarts - scheduler configurations
# ------------------------------------------------
n_cycle: 1          # n_cycle for total training epochs
warmup_ratio: 0.15  # decides steps to warmup
cycle_mult: 1.
max_lr: 0.000005
min_lr: 0.000005
gamma: 0.75

# ------------------------------------------------
# Training data configuration
# ------------------------------------------------
sample_rate: 16000
data_path: '../data/VoxCeleb'   # about training set
training_set: 'voxceleb2-dev'
n_train_class: 5994
speed_perturb: [1.0]            # about speaker augmentation
musan_path: '../data/MUSAN' 
rir_path: '../data/RIRs'
p_augment: 0.6
